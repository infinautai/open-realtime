# OpenRealtime API Service - Real-time Voice Language Model Server

A WebSocket-based real-time voice conversation system that supports voice input, intelligent text generation, and voice output for complete conversational workflows.

## ğŸ¯ Project Background

In the past half year, omni-modal models and speech-to-speech models such as Qwen Omni, Ming-Omni, Qwen Audio and Kimi Audio have been released one after another, highlighting that omni-modal models are currently a major focus in AI research. However, there is still a lack of open-source tools for real-time API interaction.

This project is inspired by the OpenAI realtime API and playground, and aims to provide an open-source version of a realtime playground. We hope this tool can help accelerate the adoption and development of these advanced models in the community.

## âœ¨ Features

- ğŸ¤ **Real-time Voice Input**: Supports streaming audio data processing and real-time voice activity detection (VAD)
- ğŸ¤– **Intelligent Conversation**: Integrates large language models with support for multimodal input (text + audio)
- ğŸ”Š **Voice Synthesis**: Supports multiple TTS services (OpenAI TTS, ElevenLabs)
- ğŸŒ **WebSocket Real-time Communication**: Low-latency bidirectional communication with event-driven architecture
- ğŸ“ **Speech-to-Text**: High-precision speech recognition based on Whisper models
- ğŸ”„ **Streaming Processing**: Supports streaming generation of text and audio, optimizing response latency
- ğŸ›ï¸ **Configurable**: Supports multiple audio formats, sample rates, and model parameter adjustments

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    WebSocket    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   FastAPI Server â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€orâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€orâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                        â”‚                        â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   STT Engine   â”‚    â”‚    LLM Engine    â”‚    â”‚     TTS Engine     â”‚
           â”‚   (Whisper)    â”‚    â”‚  (Qwen2.5-Omni) â”‚    â”‚ (OpenAI/ElevenLabs)â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                        â”‚                        â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Speech Recognitionâ”‚  â”‚  Text Generation â”‚    â”‚  Speech Synthesis  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```




## ğŸš€ Quick Start

### Requirements

- Python 3.12+
- macOS/Linux (recommended)
- Sufficient memory for loading models

### Install Dependencies

```bash
# Clone the project
git clone <repository-url>
cd realtime_llm

# Install dependencies
pip install -r requirements.txt

# Or use uv (recommended)
uv sync
```

### Environment Configuration

Create a `.env` file:

```bash
# OpenAI API configuration (for TTS)
OPENAI_API_KEY=your_openai_api_key

# ElevenLabs API configuration (optional)
ELEVENLABS_API_KEY=your_elevenlabs_api_key

# Server configuration
HOST=0.0.0.0
PORT=7866
```

### Start the Server

```bash
# Start with default configuration
python main.py

# Or specify parameters
python main.py --host 0.0.0.0 --port 7866

# Development mode (auto-reload)
python main.py --reload
```

After the server starts, the WebSocket endpoint is: `ws://localhost:7866/realtime`

## ğŸ–¥ï¸ Client Example

We provide a complete web client example that can interact directly with this server:

**ğŸ”— Client Project**: [Realtime Playground](https://github.com/infinautai/realtime-playground)

The client provides:
- ğŸ¤ Real-time voice recording and playback
- ğŸ’¬ WebSocket connection management
- ğŸ›ï¸ Real-time parameter adjustment interface
- ğŸ“Š Audio visualization
- ğŸ”§ Complete debugging tools

## ğŸ“– Usage Guide

### WebSocket Event System

The client communicates with the server through WebSocket by sending JSON-formatted events:

#### Client Events

- `session.update` - Update session configuration
- `input_audio_buffer.append` - Add audio data
- `input_audio_buffer.commit` - Commit audio buffer
- `input_audio_buffer.clear` - Clear audio buffer
- `conversation.item.create` - Create conversation item
- `response.create` - Request response generation
- `response.cancel` - Cancel response generation

#### Server Events

- `session.created` - Session created successfully
- `session.updated` - Session configuration updated
- `input_audio_buffer.speech_started` - Speech detected started
- `input_audio_buffer.speech_stopped` - Speech detected stopped
- `response.text.delta` - Text response delta
- `response.audio.delta` - Audio response delta
- `response.done` - Response generation completed

### Supported Audio Formats

- **PCM16** (16kHz, 16-bit)
- **G711 Î¼-law** (8kHz)
- **G711 A-law** (8kHz)

### Example: Basic Conversation Flow

```javascript
// 1. Establish WebSocket connection
const ws = new WebSocket('ws://localhost:7866/realtime');

// 2. Update session configuration
ws.send(JSON.stringify({
    type: 'session.update',
    session: {
        modalities: ['text', 'audio'],
        instructions: 'You are a friendly AI assistant',
        turn_detection: {
            type: 'server_vad',
            threshold: 0.5,
            prefix_padding_ms: 300,
            silence_duration_ms: 800
        }
    }
}));

// 3. Send audio data
ws.send(JSON.stringify({
    type: 'input_audio_buffer.append',
    audio: base64AudioData
}));

// 4. Listen for responses
ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    console.log('Received event:', data.type);
};
```

## ğŸ”§ Configuration Options

### Session Configuration

```json
{
    "modalities": ["text", "audio"],
    "instructions": "System prompt",
    "input_audio_format": "pcm16",
    "output_audio_format": "pcm16",
    "input_audio_transcription": {
        "model": "whisper-1"
    },
    "turn_detection": {
        "type": "server_vad",
        "threshold": 0.5,
        "prefix_padding_ms": 300,
        "silence_duration_ms": 800
    },
    "temperature": 0.8,
    "max_response_output_tokens": 4096
}
```

### VAD Parameter Tuning

- `threshold`: Voice detection threshold (0.0-1.0)
- `prefix_padding_ms`: Pre-speech buffer time
- `silence_duration_ms`: Silence detection duration

### TTS Engine Configuration

**OpenAI TTS:**
```json
{
    "voice": "alloy",  // alloy, echo, fable, onyx, nova, shimmer
    "model": "gpt-4o-mini-tts",
    "sample_rate": 16000
}
```

**ElevenLabs:**
```json
{
    "voice": "voice_id",
    "model": "eleven_multilingual_v2",
    "sample_rate": 16000
}
```

## ğŸƒâ€â™‚ï¸ Development and Testing

### Run Tests

```bash
# TTS engine tests
python tts/test_tts_engine.py

# Unit tests
python -m pytest utils/test_*.py
```

### Development Mode

The project supports hot-reload development:

```bash
python main.py --reload
```

### Mock Mode

For development and testing without real models:

```python
# In server.py
from engine.mock_engine import MockLLMEngine as QwenOmniLLMEngine
```

## ğŸ“ Project Structure

```
realtime_llm/
â”œâ”€â”€ audio/                   # Audio processing modules
â”‚   â”œâ”€â”€ resamplers/         # Audio resamplers
â”‚   â””â”€â”€ vad/               # Voice activity detection
â”œâ”€â”€ engine/                 # LLM engine implementations
â”‚   â”œâ”€â”€ mock_engine.py     # Mock engine (for testing)
â”‚   â””â”€â”€ qwen_omni.py       # Qwen multimodal engine
â”œâ”€â”€ stt/                   # Speech-to-text
â”‚   â””â”€â”€ whisper.py         # Whisper STT implementation
â”œâ”€â”€ tts/                   # Text-to-speech
â”‚   â”œâ”€â”€ openai_tts.py      # OpenAI TTS service
â”‚   â””â”€â”€ elevenlabs_tts.py  # ElevenLabs TTS service
â”œâ”€â”€ utils/                 # Utility functions
â”œâ”€â”€ events.py              # Event definitions
â”œâ”€â”€ session.py             # Session management
â”œâ”€â”€ server.py              # FastAPI server
â””â”€â”€ main.py               # Application entry point
```

## ğŸ”§ Deployment

### Docker Deployment

```dockerfile
FROM python:3.12-slim

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt

EXPOSE 7866

CMD ["python", "main.py", "--host", "0.0.0.0", "--port", "7866"]
```

### Production Environment Recommendations

- Use reverse proxy (Nginx) to handle WebSocket connections
- Configure appropriate resource limits
- Enable logging and monitoring
- Use process managers (systemd, supervisord)

## ğŸ¤ Contributing

Issues and Pull Requests are welcome!

### Development Guidelines

- Use type annotations
- Follow PEP 8 code style
- Add appropriate logging
- Write test cases

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Troubleshooting

### Common Issues

**Q: Voice detection is inaccurate?**
A: Adjust VAD parameters, especially `threshold` and `silence_duration_ms`

**Q: Poor audio quality?**
A: Check sample rate settings, ensure client and server sample rates match

**Q: High response latency?**
A: Optimize network connection, consider using faster TTS services or local deployment

**Q: High memory usage?**
A: Adjust model parameters, use smaller models or increase system memory

### Debug Mode

Enable verbose logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

ğŸ“§ For questions, please submit an Issue or contact the maintainers.